# ğŸš¦ LLM Eval Guard

![Python](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)

**Prevent silent AI failures before they hit production**

LLM Eval Guard is a lightweight, production-minded evaluation framework that detects quality regressions in LLM outputs when prompts, datasets, or models change. 

---

## ğŸ”¥ Why This Exists

In real products, prompts evolve, models update, and teams "optimize" responses â€” but most teams don't test LLM outputs like code.  Failures slip into production silently. 

**This project answers one question:  Did this change make the AI worse?**

| Failure Type | Description |
|--------------|-------------|
| ğŸ”» Information Loss | Critical details missing from responses |
| ğŸ“‰ Over-simplification | Incomplete or shallow answers |
| ğŸ­ Hallucinations | Fabricated technical details |
| ğŸ”„ Behavioral Drift | Inconsistent behavior after updates |

---

## ğŸ§­ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Prompt v1â”‚           â”‚Prompt v2â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â–¼                     â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   LLM   â”‚           â”‚   LLM   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â–¼                     â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Validatorsâ”‚         â”‚Validatorsâ”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â–¼                    â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Score: 3â”‚           â”‚ Score: 2â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Regression   â”‚
          â”‚    Detected!   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… What The Framework Does

- âœ”ï¸ Runs the same dataset across multiple prompt versions
- âœ”ï¸ Evaluates outputs using deterministic validators
- âœ”ï¸ Scores responses objectively
- âœ”ï¸ Flags regressions with structured JSON reports
- âœ”ï¸ Works with cloud + local models (Ollama supported)

---

## ğŸ§  Evaluation Signals

The framework validates responses for: 

- **Minimum completeness** â€” Is the response substantive?
- **Required domain keywords** â€” Does it cover expected concepts?
- **Refusal / non-answer patterns** â€” Did the model decline to answer?
- **Hallucinated entities** â€” Are there ungrounded claims? 
- **Regression score** â€” Is v2 measurably worse than v1? 

### Example Validator

Validators are simple and hackable:

```python
# validators/keywords.py
REQUIRED = ["authentication", "authorization", "token"]

def validate(response_text):
    text = response_text.lower()
    matches = sum(1 for keyword in REQUIRED if keyword in text)
    return {
        "passed": matches == len(REQUIRED),
        "score": matches,
        "max_score": len(REQUIRED),
        "missing": [k for k in REQUIRED if k not in text]
    }
```

---

## ğŸ“Š Example Regression Result

```json
{
  "id": 2,
  "regression":  true,
  "v1_score": { "score": 3, "max_score": 4 },
  "v2_score": { "score": 2, "max_score": 4 }
}
```

**Interpretation:** Prompt v2 produced a weaker answer than v1 â†’ regression detected. 

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Deterministic custom validators**
- **Supported LLM Providers:**
  - OpenAI
  - Google Gemini
  - Ollama (local LLMs)
- **CI-driven evaluation workflow**

---

## â–¶ï¸ Getting Started

### Installation

```bash
pip install -r requirements.txt
```

### Run Evaluation

```bash
python -m runner.run_eval
```

### Output

Reports are saved to: 
```
/reports/latest_report.json
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ eval_dataset.json     # Test cases
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ v1.txt                # Baseline prompt
â”‚   â””â”€â”€ v2.txt                # Updated prompt
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ length.py             # Length / completeness checks
â”‚   â”œâ”€â”€ keywords.py           # Domain keyword coverage
â”‚   â”œâ”€â”€ refusal.py            # Non-answer detection
â”‚   â””â”€â”€ hallucination.py      # Ungrounded claim detection
â”œâ”€â”€ runner/
â”‚   â””â”€â”€ run_eval.py           # Evaluation runner
â””â”€â”€ reports/
    â””â”€â”€ latest_report.json    # Generated reports
```

---

## ğŸ”„ CI Integration

This repo includes GitHub Actions CI that automatically runs evaluation when:

- Prompts change
- Datasets change
- Validators change

**This prevents unnoticed regressions from entering `main`.**

CI enforces evaluation discipline without relying on manual review.

---

## ğŸ¯ Real-World Use Cases

- **AI Feature QA** â€” Validate outputs before releases
- **Prompt Engineering Quality Gates** â€” Ensure prompt changes don't degrade quality
- **Model Upgrade Regression Checks** â€” Test new model versions safely
- **Enterprise Reliability Workflows** â€” Build trust in AI systems

---

## ğŸš€ Roadmap

- [ ] **Fail CI if regression score drops below threshold** â† Quality gates
- [ ] Human review mode for flagged edge cases
- [ ] Baseline locking for enterprise audit trails
- [ ] Richer validators (JSON/schema validation)
- [ ] Scoring dashboards with historical trends
- [ ] Multi-model comparison reports

---

## ğŸ“„ License

Apache License 2.0

---

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR. 